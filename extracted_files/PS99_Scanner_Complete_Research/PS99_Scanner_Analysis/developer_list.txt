    DEVELOPER_IDS, GROUP_IDS, GAME_IDS, STUDIO_SOURCES, 
    ASSET_TYPES, PS99_KEYWORDS, SCAN_INTERVAL, HISTORY_DAYS,
    RESULTS_PER_PAGE, THUMBNAIL_CACHE_DIR, NOTIFICATION_SOUND,
    ENABLE_DESKTOP_NOTIFICATIONS, ENABLE_SOUND_NOTIFICATIONS,
    ENABLE_VISUAL_HIGHLIGHTS, ENABLE_AUTO_CATEGORIZATION,
    CATEGORIZATION_RULES, DEFAULT_SETTINGS, USE_WEB_DOMAIN_FALLBACK
)

# Import enhanced search keywords for Anubis update and developer blogs
from search_keywords import (
    ANUBIS_KEYWORDS, DEVELOPER_TESTING_KEYWORDS, DEVELOPER_BLOG_KEYWORDS,
    ALL_ENHANCED_SEARCH_TERMS
)

# This function has been moved to main.py for better integration

class ComprehensiveRobloxScanner:
    """
    Advanced scanner for monitoring Roblox assets across multiple sources
    with real-time updates and history tracking.
    """
    
    def __init__(self, cache_dir=THUMBNAIL_CACHE_DIR, 
                 database_path="ps99_assets.db",
                 settings=None):
        """
        Initialize the scanner with the specified cache directory and database.
        
        Args:
            cache_dir: Directory to store downloaded thumbnails
            database_path: Path to the SQLite database for asset history
--
                                if developer_id in DEVELOPER_IDS:
                                    is_ps99_related = True
                                
                                if is_ps99_related:
                                    assets.append({
                                        'id': str(asset_id),
                                        'name': asset_data.get('Name', 'Unnamed Asset'),
                                        'description': asset_data.get('Description', ''),
                                        'type': asset_data.get('AssetType', 'Unknown'),
                                        'creator_id': str(developer_id),
                                        'creator_name': username,
                                        'source_type': 'developer',
                                        'source_id': str(developer_id),
                                        'source_name': username
                                    })
                        except Exception as e:
                            logger.error(f"Error getting asset details for {asset_id}: {e}")
        
        except Exception as e:
            logger.error(f"Error scanning developer assets: {e}")
        
        return assets
    
    def _scan_group(self, group_id):
        """
        Scan a group's assets for new content.
        
        Args:
            group_id: The Roblox group ID
            
        Returns:
--
        logger.info(f"SCANNING: Number of developers to scan: {len(DEVELOPER_IDS)}")
        logger.info(f"SCANNING: Developer IDs: {DEVELOPER_IDS}")
        logger.info(f"SCANNING: Number of groups to scan: {len(GROUP_IDS)}")
        logger.info(f"SCANNING: Group IDs: {GROUP_IDS}")
        logger.info(f"SCANNING: Number of games to scan: {len(GAME_IDS)}")
        logger.info(f"SCANNING: Game IDs: {GAME_IDS}")
        # Try to get additional developer IDs from web domain fallback if enabled
        if USE_WEB_DOMAIN_FALLBACK:
            try:
                # Get additional developer IDs
                web_developers = web_domain_client.get_developer_ids()
                if web_developers:
                    logger.info(f"Adding {len(web_developers)} additional developer IDs from web domain")
                    DEVELOPER_IDS.extend([dev_id for dev_id in web_developers if dev_id not in DEVELOPER_IDS])
                
                # Get additional group IDs
                web_groups = web_domain_client.get_group_ids()
                if web_groups:
                    logger.info(f"Adding {len(web_groups)} additional group IDs from web domain")
                    GROUP_IDS.extend([group_id for group_id in web_groups if group_id not in GROUP_IDS])
                
                # Get additional game IDs
                web_games = web_domain_client.get_game_ids()
                if web_games:
                    logger.info(f"Adding {len(web_games)} additional game IDs from web domain")
                    GAME_IDS.extend([game_id for game_id in web_games if game_id not in GAME_IDS])
            except Exception as e:
                logger.error(f"Error fetching additional IDs from web domain: {e}")
        all_assets = []
        new_assets = []
        
        with self.lock:
            # Scan developers
            for developer_id in DEVELOPER_IDS:
                assets = self._scan_developer(developer_id)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(2)
            
            # Scan groups
            for group_id in GROUP_IDS:
                assets = self._scan_group(group_id)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(2)
            
            # Scan games
            for game_id in GAME_IDS:
                assets = self._scan_game(game_id)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(2)
            
            # Scan marketplace
            for url in STUDIO_SOURCES:
                assets = self._scan_marketplace(url)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(1)
            
            # Scan custom sources
            custom_sources = self.get_custom_sources()
            for source in custom_sources:
                if source['enabled']:
--
            'monitored_developers': len(DEVELOPER_IDS),
            'monitored_groups': len(GROUP_IDS),
            'monitored_games': len(GAME_IDS),
            'custom_sources': len(self.get_custom_sources())
        }
    
    def search_assets(self, query, page=1, per_page=RESULTS_PER_PAGE):
        """
        Search assets by keyword.
        
        Args:
            query: Search query
            page: Page number for pagination
            per_page: Number of results per page
            
        Returns:
            Dictionary with search results and pagination info
        """
        conn = sqlite3.connect(self.database_path)
        cursor = conn.cursor()
        
        # Build query
        search_query = '''
        SELECT id, name, description, asset_type, creator_name, timestamp, thumbnail_path, source_name, category, is_new 
        FROM assets 
        WHERE name LIKE ? OR description LIKE ?
        ORDER BY timestamp DESC
        LIMIT ? OFFSET ?
        '''
        
        search_params = [f"%{query}%", f"%{query}%", per_page, (page - 1) * per_page]
