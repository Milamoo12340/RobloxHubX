    DEVELOPER_IDS, GROUP_IDS, GAME_IDS, STUDIO_SOURCES, 
    ASSET_TYPES, PS99_KEYWORDS, SCAN_INTERVAL, HISTORY_DAYS,
    RESULTS_PER_PAGE, THUMBNAIL_CACHE_DIR, NOTIFICATION_SOUND,
    ENABLE_DESKTOP_NOTIFICATIONS, ENABLE_SOUND_NOTIFICATIONS,
    ENABLE_VISUAL_HIGHLIGHTS, ENABLE_AUTO_CATEGORIZATION,
    CATEGORIZATION_RULES, DEFAULT_SETTINGS, USE_WEB_DOMAIN_FALLBACK
)

# Import enhanced search keywords for Anubis update and developer blogs
from search_keywords import (
    ANUBIS_KEYWORDS, DEVELOPER_TESTING_KEYWORDS, DEVELOPER_BLOG_KEYWORDS,
    ALL_ENHANCED_SEARCH_TERMS
)

# This function has been moved to main.py for better integration

class ComprehensiveRobloxScanner:
    """
    Advanced scanner for monitoring Roblox assets across multiple sources
    with real-time updates and history tracking.
    """
--
        logger.info(f"SCANNING: Number of games to scan: {len(GAME_IDS)}")
        logger.info(f"SCANNING: Game IDs: {GAME_IDS}")
        # Try to get additional developer IDs from web domain fallback if enabled
        if USE_WEB_DOMAIN_FALLBACK:
            try:
                # Get additional developer IDs
                web_developers = web_domain_client.get_developer_ids()
                if web_developers:
                    logger.info(f"Adding {len(web_developers)} additional developer IDs from web domain")
                    DEVELOPER_IDS.extend([dev_id for dev_id in web_developers if dev_id not in DEVELOPER_IDS])
                
                # Get additional group IDs
                web_groups = web_domain_client.get_group_ids()
                if web_groups:
                    logger.info(f"Adding {len(web_groups)} additional group IDs from web domain")
                    GROUP_IDS.extend([group_id for group_id in web_groups if group_id not in GROUP_IDS])
                
                # Get additional game IDs
                web_games = web_domain_client.get_game_ids()
                if web_games:
                    logger.info(f"Adding {len(web_games)} additional game IDs from web domain")
                    GAME_IDS.extend([game_id for game_id in web_games if game_id not in GAME_IDS])
            except Exception as e:
                logger.error(f"Error fetching additional IDs from web domain: {e}")
        all_assets = []
        new_assets = []
        
        with self.lock:
            # Scan developers
            for developer_id in DEVELOPER_IDS:
                assets = self._scan_developer(developer_id)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(2)
            
            # Scan groups
            for group_id in GROUP_IDS:
                assets = self._scan_group(group_id)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(2)
            
--
            for game_id in GAME_IDS:
                assets = self._scan_game(game_id)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(2)
            
            # Scan marketplace
            for url in STUDIO_SOURCES:
                assets = self._scan_marketplace(url)
                all_assets.extend(assets)
                # Add delay to avoid rate limiting
                time.sleep(1)
            
            # Scan custom sources
            custom_sources = self.get_custom_sources()
            for source in custom_sources:
                if source['enabled']:
                    assets = self._scan_custom_source(source['type'], source['value'], source['name'])
                    all_assets.extend(assets)
                    # Add delay to avoid rate limiting
                    time.sleep(1)
--
            'monitored_games': len(GAME_IDS),
            'custom_sources': len(self.get_custom_sources())
        }
    
    def search_assets(self, query, page=1, per_page=RESULTS_PER_PAGE):
        """
        Search assets by keyword.
        
        Args:
            query: Search query
            page: Page number for pagination
            per_page: Number of results per page
            
        Returns:
            Dictionary with search results and pagination info
        """
        conn = sqlite3.connect(self.database_path)
        cursor = conn.cursor()
        
        # Build query
        search_query = '''
